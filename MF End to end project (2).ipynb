{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8483e02-7c11-4b8e-b5e3-44d777a843bc",
   "metadata": {},
   "source": [
    "### Importing Libraries and Configuring the Setup\n",
    "Here I bring in all the libraries I’ll need — pandas, numpy, matplotlib, etc. — for data handling, visualization, and optimization.\n",
    "I also set up file paths and define rules like maximum stock weight and the tilt towards Consumer and Finance sectors during festive months.\n",
    "This is the backbone of the project — once this is set, the rest of the analysis flows smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f25323-6596-4049-840d-312ac931fb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell A - imports & config\n",
    "import os, math, warnings\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "# Config (tweak)\n",
    "DATA_DIR = \"data/prices\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "EXCEL_PATH = \"Tickers.csv\"   # path to your NSE file screenshot showed\n",
    "MAX_WEIGHT_PER_STOCK = 0.10\n",
    "ANNUALIZE = 252\n",
    "TILT_MONTHS = [9,10,11,12]   # Sep-Dec (festive season)\n",
    "TILT_SHARE = 0.12            # increase total allocation to target sectors by this absolute share (e.g., 0.12 = +12%)\n",
    "TARGET_SECTORS = [\"Consumer\",\"Finance\"]  # sector names to tilt toward\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c39d87-c5e1-4748-96a6-e2caca8a476b",
   "metadata": {},
   "source": [
    "###  Loading Tickers\n",
    "This part is about loading the tickers from a CSV file.  \n",
    "I also add some error handling so the code doesn’t crash if the file is missing or misnamed.  \n",
    "It’s a small step, but important because without a clean list of tickers, nothing else can move forward.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d548872f-d1f5-44f9-a269-96ba77200269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell B: Read tickers safely\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_tickers():\n",
    "    try:\n",
    "        # Always read from tickers.csv\n",
    "        df = pd.read_csv(\"tickers.csv\")\n",
    "        print(f\"[ok] Loaded {len(df)} tickers from tickers.csv\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"tickers.csv not found! Please save the file in the same folder.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Unexpected error reading tickers.csv: {e}\")\n",
    "\n",
    "# Test\n",
    "tickers_df = load_tickers()\n",
    "display(tickers_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f991fa-2344-401f-8eaa-6edb44ad29ff",
   "metadata": {},
   "source": [
    "### Cleaning and Preparing Ticker Data\n",
    "This step standardizes the ticker symbols by removing any unwanted suffixes (like `.NS`) and ensures consistency.  \n",
    "The cleaned data is saved back for future steps. Having clean ticker data is critical because mismatches can lead to errors when merging or fetching stock prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b228946-3c2d-4017-8c57-0131755a06de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your tickers file\n",
    "tickers_df = pd.read_csv(EXCEL_PATH)  # or pd.read_excel() if you're using Excel\n",
    "\n",
    "# ✅ Clean column names (remove \\n, tabs, extra spaces)\n",
    "tickers_df.columns = (\n",
    "    tickers_df.columns\n",
    "    .str.replace(r'\\s+', ' ', regex=True)  # Replace \\n, \\t, and multiple spaces with a single space\n",
    "    .str.strip()                           # Remove leading/trailing spaces\n",
    ")\n",
    "\n",
    "tickers_df['TICKER'] = tickers_df.iloc[:, 0].astype(str).str.strip() + \".NS\"\n",
    "tickers_df.to_csv(EXCEL_PATH, index=False)\n",
    "\n",
    "\n",
    "display(tickers_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee0d804-4026-4c45-a1e1-f03a8cc68011",
   "metadata": {},
   "source": [
    "### Cell 4: Adding Sector Information\n",
    "I link the tickers with their sector data and remove entries like “NIFTY 100” since that’s not a tradable stock.  \n",
    "This step matters because my whole project revolves around sector tilts, so I need every stock tied to the right sector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cbd260-30c3-422d-b7be-742c66dcf41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_df = pd.read_csv(\"Tickers.csv\")\n",
    "sectors_df = pd.read_csv(\"sectors.csv\")\n",
    "\n",
    "# Clean tickers_df column names: remove newline and extra whitespace\n",
    "tickers_df.columns = tickers_df.columns.str.replace('\\n', '', regex=True).str.strip()\n",
    "\n",
    "# Clean sectors_df columns similarly (just in case)\n",
    "sectors_df.columns = sectors_df.columns.str.replace('\\n', '', regex=True).str.strip()\n",
    "\n",
    "# At this point, tickers_df.columns should include 'SYMBOL' exactly\n",
    "print(\"After cleaning tickers:\", tickers_df.columns.tolist())\n",
    "\n",
    "\n",
    "# Then drop 'NIFTY 100'\n",
    "tickers_df = tickers_df[tickers_df['Symbol'] != 'NIFTY 100']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9028a2d-9683-4339-8b0e-a2dd8bb3617f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tickers_df['Industry'].value_counts().head(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7d3ac4-0264-4dd1-b0f9-8cecfd714df5",
   "metadata": {},
   "source": [
    "###  Checking Industry Spread\n",
    "Here I quickly check how many companies belong to each industry.  \n",
    "It helps me see if Consumer and Finance are well represented — because if they’re not, tilting towards them wouldn’t mean much.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6c2701-d66e-4c57-a6b2-157b46bc1d3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Example: Top Indian stocks (same as in your dataset)\n",
    "stocks = [\"RELIANCE.NS\", \"HDFCBANK.NS\", \"ICICIBANK.NS\",\n",
    "          \"INFY.NS\", \"TCS.NS\", \"KOTAKBANK.NS\",\n",
    "          \"HINDUNILVR.NS\", \"ITC.NS\", \"LT.NS\", \"SBIN.NS\"]\n",
    "\n",
    "# Fetch data from Jan 2022 till today\n",
    "data = yf.download(stocks, start=\"2019-01-01\", end=datetime.today().strftime('%Y-%m-%d'))['Close']\n",
    "\n",
    "# Reset index to keep Date column\n",
    "data = data.reset_index()\n",
    "print(data.tail())   # check latest rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbddd33d-351e-425c-bc99-f3f8f31e8935",
   "metadata": {},
   "source": [
    "###  Testing Price Downloads\n",
    "Before I go all in, I test downloading price data for a few well-known Indian stocks.  \n",
    "This shows me that Yahoo Finance is working as expected and I can get the historical data I’ll need.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacfd3e4-a229-4bdb-b7b9-f911dfcf76ac",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e05c09-6db7-4256-87fa-1d6d0e95bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_prices(data_dir=DATA_DIR):\n",
    "    price_files = glob(os.path.join(data_dir, \"*.parquet\"))\n",
    "    price_data = []\n",
    "\n",
    "    for file in price_files:\n",
    "        try:\n",
    "            df = pd.read_parquet(file)\n",
    "            print(f\"Columns in {file}: {df.columns.tolist()}\")  # Debug\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"Skipping {file}, dataframe empty\")\n",
    "                continue\n",
    "\n",
    "            if 'Close' not in df.columns:\n",
    "                print(f\"Skipping {file}, no 'Close' column\")\n",
    "                continue\n",
    "\n",
    "            ticker = os.path.basename(file).replace(\".parquet\", \"\").replace(\"_\", \".\")\n",
    "            \n",
    "            temp_df = df[['Close']].copy()\n",
    "            temp_df['Ticker'] = ticker\n",
    "            temp_df['Date'] = temp_df.index\n",
    "            temp_df.rename(columns={'Close': 'Price'}, inplace=True)\n",
    "\n",
    "            # Check columns after rename\n",
    "            print(f\"Columns in temp_df before append: {temp_df.columns.tolist()}\")\n",
    "\n",
    "            price_data.append(temp_df)\n",
    "        except Exception as e:\n",
    "            print(f\"[error] Failed to load {file}: {e}\")\n",
    "\n",
    "    if price_data:\n",
    "        all_prices = pd.concat(price_data, ignore_index=True)\n",
    "        print(\"Columns after concat:\", all_prices.columns.tolist())  # Debug\n",
    "        \n",
    "        if 'Price' not in all_prices.columns:\n",
    "            print(\"ERROR: 'Price' column not found in concatenated DataFrame\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        all_prices['Date'] = pd.to_datetime(all_prices['Date'])\n",
    "        \n",
    "        # Print a sample before dropping NA\n",
    "        print(\"Sample before dropping NA:\")\n",
    "        print(all_prices.head())\n",
    "\n",
    "        all_prices.dropna(subset=['Price'], inplace=True)\n",
    "\n",
    "        print(\"Sample after dropping NA:\")\n",
    "        print(all_prices.head())\n",
    "        \n",
    "        return all_prices\n",
    "    else:\n",
    "        print(\"No valid price data found.\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8ae66-8c9a-4792-9ab2-38b73d28e01e",
   "metadata": {},
   "source": [
    "###  Building a Price Loader\n",
    "This function goes through all the stored parquet files, picks out the prices, and combines them into one dataset.  \n",
    "Instead of dealing with scattered files, I now have everything in a single, clean DataFrame — which is much easier to work with.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6efb4-cc29-4b7e-bdec-d94ad3001bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Fetch sample data\n",
    "df = yf.download(\"AAPL\", start=\"2023-01-01\", end=\"2023-01-10\")\n",
    "\n",
    "# Show the columns\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5f877d-ea23-4a96-af96-8b51d4710176",
   "metadata": {},
   "source": [
    "###  Checking Yahoo Finance Data Format\n",
    "I quickly download data for Apple (AAPL) just to see the structure of Yahoo Finance data.  \n",
    "It’s a sanity check to confirm I know which columns to use, especially the “Close” price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7e2b9-f247-4e92-85ea-25d983467d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file = \"data/prices/ABB_NS.parquet\"\n",
    "df = pd.read_parquet(file)\n",
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc85c93-c1e4-4d50-a7f5-a57e34dcddcf",
   "metadata": {},
   "source": [
    "###  Looking Inside a Local File\n",
    "Here I open one of my stored parquet files to make sure it’s in the right format.  \n",
    "This way, I don’t waste time running the full pipeline only to discover later that the files were saved incorrectly.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5534ddd0-1373-4ef0-8449-8748c7eb5396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "def load_all_prices(data_dir):\n",
    "    price_files = glob(os.path.join(data_dir, \"*.parquet\"))\n",
    "    price_data = []\n",
    "\n",
    "    print(f\"Looking in: {data_dir}\")\n",
    "    print(f\"Found parquet files: {price_files}\")\n",
    "\n",
    "    for file in price_files:\n",
    "        try:\n",
    "            df = pd.read_parquet(file)\n",
    "            print(f\"Columns in {file}: {df.columns}\")  # MultiIndex or not?\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"Skipping {file}, dataframe empty\")\n",
    "                continue\n",
    "\n",
    "            # Handle MultiIndex columns\n",
    "            if isinstance(df.columns, pd.MultiIndex):\n",
    "                # Get ticker name from columns (second level of MultiIndex)\n",
    "                tickers_in_file = list(set([col[1] for col in df.columns if col[0] == 'Close']))\n",
    "                if not tickers_in_file:\n",
    "                    print(f\"Skipping {file}, no 'Close' column in MultiIndex\")\n",
    "                    continue\n",
    "\n",
    "                ticker = tickers_in_file[0]  # Assuming one ticker per file\n",
    "                df_close = df[('Close', ticker)].copy().to_frame(name='Price')\n",
    "\n",
    "            else:\n",
    "                if 'Close' not in df.columns:\n",
    "                    print(f\"Skipping {file}, no 'Close' column\")\n",
    "                    continue\n",
    "                df_close = df[['Close']].copy()\n",
    "                df_close.rename(columns={'Close': 'Price'}, inplace=True)\n",
    "\n",
    "                # Extract ticker from filename\n",
    "                ticker = os.path.basename(file).replace(\".parquet\", \"\").replace(\"_\", \".\")\n",
    "\n",
    "            df_close['Ticker'] = ticker\n",
    "            df_close['Date'] = df_close.index\n",
    "\n",
    "            print(f\"Processed {file} -> Columns: {df_close.columns.tolist()}\")\n",
    "            price_data.append(df_close)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[error] Failed to load {file}: {e}\")\n",
    "\n",
    "    if price_data:\n",
    "        all_prices = pd.concat(price_data, ignore_index=True)\n",
    "        all_prices['Date'] = pd.to_datetime(all_prices['Date'])\n",
    "        all_prices.dropna(subset=['Price'], inplace=True)\n",
    "\n",
    "        print(\"Final sample:\")\n",
    "        print(all_prices.head())\n",
    "\n",
    "        return all_prices\n",
    "    else:\n",
    "        print(\"No valid price data found.\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf68722-3735-4450-8960-eb99d7bbc026",
   "metadata": {},
   "source": [
    "###  Improving the Price Loader\n",
    "I upgrade my price loader so it can handle different file formats, including ones with MultiIndex columns.  \n",
    "This makes the pipeline more robust — no matter how the data is stored, I’ll still be able to pull out the prices I need.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5957dd-ee44-4600-b10f-9b4a5f77dbaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_all = load_all_prices(DATA_DIR)\n",
    "print(df_all.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41eced0-7850-44fc-be5e-7a7e84f74a93",
   "metadata": {},
   "source": [
    "###  Loading All Price Data\n",
    "In this step, I loaded all the stored price data from the parquet files into a single DataFrame using the `load_all_prices()` function.  \n",
    "This gave me one consolidated dataset (`df_all`) where each ticker’s historical prices were available in a consistent format.  \n",
    "By printing the first few rows, I confirmed that the loading process worked correctly and that the dataset structure was as expected before moving forward.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a73184-bd4f-46b9-8307-86a7d7f4b59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure TICKER and Ticker columns are string\n",
    "tickers_df['TICKER'] = tickers_df['TICKER'].astype(str)\n",
    "df_all['Ticker'] = df_all['Ticker'].astype(str)\n",
    "\n",
    "# Create mapping dictionaries\n",
    "ticker_to_sector = tickers_df.set_index('TICKER')['Industry'].to_dict()\n",
    "ticker_to_name = tickers_df.set_index('TICKER')['Company Name'].to_dict()\n",
    "\n",
    "# Map to df_all\n",
    "df_all['Sector'] = df_all['Ticker'].map(ticker_to_sector)\n",
    "df_all['Company Name'] = df_all['Ticker'].map(ticker_to_name)\n",
    "\n",
    "# Check for tickers that didn't get mapped\n",
    "missing_sector = df_all[df_all['Sector'].isna()]['Ticker'].unique()\n",
    "missing_name = df_all[df_all['Company Name'].isna()]['Ticker'].unique()\n",
    "\n",
    "if len(missing_sector) > 0 or len(missing_name) > 0:\n",
    "    print(\"Tickers missing sector or company name info:\")\n",
    "    print(\"Missing Sector:\", missing_sector)\n",
    "    print(\"Missing Company Name:\", missing_name)\n",
    "else:\n",
    "    print(\"✅ All tickers successfully mapped to sector and company name.\")\n",
    "\n",
    "# Optional: check a sample\n",
    "print(df_all[['Ticker', 'Sector', 'Company Name','Price']].drop_duplicates().tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dddb556-f3de-4e59-89dc-d2c52c018a8c",
   "metadata": {},
   "source": [
    "###  Mapping Tickers to Sector and Company Names\n",
    "In this step, I ensured that both the `TICKER` and `Ticker` columns were stored as strings to avoid mismatches during merging.  \n",
    "I then created mapping dictionaries to connect each ticker with its corresponding sector (industry) and company name from the reference dataset.  \n",
    "After applying these mappings to the main DataFrame, I checked for any tickers that failed to map correctly.  \n",
    "This validation confirmed that almost all tickers were enriched with the correct sector and company information, which was essential for the later sector-based analysis and portfolio tilting.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38edc983-53c9-4281-8f8f-2f1709dfc433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "# Load your master tickers file\n",
    "tickers_df = pd.read_csv(\"tickers.csv\")\n",
    "\n",
    "# Create mapping\n",
    "ticker_to_sector = tickers_df.set_index('TICKER')['Industry'].to_dict()\n",
    "ticker_to_name = tickers_df.set_index('TICKER')['Company Name'].to_dict()\n",
    "\n",
    "# Loop through each parquet file and update\n",
    "for filepath in glob(os.path.join(DATA_DIR, \"*.parquet\")):\n",
    "    try:\n",
    "        df = pd.read_parquet(filepath)\n",
    "\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            # Get ticker from MultiIndex\n",
    "            tickers = [col[1] for col in df.columns if col[0] == 'Close']\n",
    "            if not tickers:\n",
    "                continue\n",
    "            ticker = tickers[0]\n",
    "            df_close = df[('Close', ticker)].copy().to_frame(name='Price')\n",
    "        else:\n",
    "            if 'Close' not in df.columns:\n",
    "                continue\n",
    "            df_close = df[['Close']].copy()\n",
    "            df_close.rename(columns={'Close': 'Price'}, inplace=True)\n",
    "            ticker = os.path.basename(filepath).replace(\".parquet\", \"\").replace(\"_\", \".\")\n",
    "\n",
    "        # Add sector and company info\n",
    "        df_close['Ticker'] = ticker\n",
    "        df_close['Sector'] = ticker_to_sector.get(ticker)\n",
    "        df_close['Company Name'] = ticker_to_name.get(ticker)\n",
    "        df_close['Date'] = df_close.index\n",
    "\n",
    "        # Save updated DataFrame to same file (overwrite)\n",
    "        df_close.to_parquet(filepath, index=True)\n",
    "        print(f\"Updated and saved: {filepath}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Could not process {filepath}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e7b60e-8a90-4454-94ef-720d93275658",
   "metadata": {},
   "source": [
    "###  Updating Parquet Files with Sector and Company Info\n",
    "Here I enriched each parquet price file with metadata about the stock’s sector and company name.  \n",
    "I first built mapping dictionaries from the master tickers file, then looped through each parquet file to extract the closing prices and associate them with the correct ticker.  \n",
    "If the file used a MultiIndex format (as Yahoo Finance sometimes provides), I handled that separately to ensure the “Close” column was correctly identified.  \n",
    "Finally, I appended the sector, company name, and date information to each dataset and overwrote the parquet files with the enriched version.  \n",
    "This ensured that all future analysis would directly have sector and company context available without requiring repeated joins or lookups.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51999add-ea1e-4821-80f3-87e1a9fcc2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sector_performance_periods(df_all):\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Define lookback periods in days approx\n",
    "    periods = {\n",
    "        '3M': 90,\n",
    "        '6M': 180,\n",
    "        '1Y': 365,\n",
    "        '3Y': 365*3\n",
    "    }\n",
    "    \n",
    "    latest_date = df_all['Date'].max()\n",
    "    results = {}\n",
    "\n",
    "    # Make sure data is sorted\n",
    "    df_all = df_all.sort_values(['Ticker', 'Date'])\n",
    "    \n",
    "    for label, days in periods.items():\n",
    "        start_date = latest_date - pd.Timedelta(days=days)\n",
    "        df_period = df_all[df_all['Date'] >= start_date]\n",
    "        \n",
    "        # Calculate cumulative return per ticker = (last price / first price) - 1\n",
    "        first_prices = df_period.groupby('Ticker').first()['Price']\n",
    "        last_prices = df_period.groupby('Ticker').last()['Price']\n",
    "        cumulative_returns = (last_prices / first_prices) - 1\n",
    "        \n",
    "        # Map sectors for tickers\n",
    "        ticker_sector = df_all.set_index('Ticker')['Sector'].drop_duplicates()\n",
    "        \n",
    "        # Combine returns & sectors into one DataFrame\n",
    "        returns_df = pd.DataFrame({\n",
    "            'Return': cumulative_returns,\n",
    "            'Sector': ticker_sector\n",
    "        }).dropna()\n",
    "        \n",
    "        # Calculate average return per sector\n",
    "        sector_returns = returns_df.groupby('Sector')['Return'].mean()\n",
    "        \n",
    "        # Convert to percentage and round to 2 decimals\n",
    "        sector_returns = (sector_returns * 100).round(2)\n",
    "        \n",
    "        results[label] = sector_returns.sort_values(ascending=False)\n",
    "    \n",
    "    # Convert results dict to DataFrame for easy viewing\n",
    "    sector_perf_df = pd.DataFrame(results)\n",
    "    \n",
    "    return sector_perf_df\n",
    "\n",
    "# Usage:\n",
    "sector_returns = sector_performance_periods(df_all)\n",
    "print(sector_returns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65044fa-4789-4950-a437-e371d625cff3",
   "metadata": {},
   "source": [
    "###  Calculating Sector Performance Over Different Periods\n",
    "In this step, I created a function to measure sector-level performance over multiple timeframes (3 months, 6 months, 1 year, and 3 years).  \n",
    "For each period, I calculated the cumulative return of every stock and then averaged these returns within each sector.  \n",
    "The results were organized into a single DataFrame where I could compare sectors side by side across different horizons.  \n",
    "This was crucial for identifying which sectors consistently outperformed and for testing whether Consumer and Finance showed strength around festive periods, directly supporting my hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2225ffda-2c71-4e7f-b245-dbcfe76216cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_sectors = sector_returns[(sector_returns['3Y'] > 30.00) & (sector_returns['3Y'] < 100.00)].sort_values(by='3Y', ascending=False)\n",
    "print(strong_sectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72cfc7e-9aef-444e-8811-949679b7edf8",
   "metadata": {},
   "source": [
    "### Filtering Strong Sectors Based on 3-Year Returns  \n",
    "\n",
    "In this step, I filtered the sectors that had delivered cumulative returns between **30% and 100% over the past 3 years**.  \n",
    "This helped narrow the analysis to industries that showed **consistent long-term growth** without being overly inflated.  \n",
    "\n",
    "The following sectors qualified as strong performers (sorted by their 3-year returns in descending order):  \n",
    "\n",
    "1. **Realty** – 94.05%  \n",
    "2. **Construction** – 85.74%  \n",
    "3. **Healthcare** – 78.40%  \n",
    "4. **Fast Moving Consumer Goods (FMCG)** – 77.31%  \n",
    "5. **Information Technology (IT)** – 71.90%  \n",
    "6. **Capital Goods** – 57.30%  \n",
    "7. **Services** – 50.38%  \n",
    "8. **Financial Services** – 33.95%  \n",
    "\n",
    "#### Why this was done  \n",
    "The aim was to focus only on **high-performing but sustainable sectors** for further exploration. By filtering out low or excessively high growth, the analysis concentrated on industries that had proven resilience and potential for continued momentum.  \n",
    "\n",
    "#### Insight Provided  \n",
    "This filtering step proved that several critical sectors like **Realty, Construction, and Healthcare** had not only survived market fluctuations but had also significantly outperformed, thereby supporting the hypothesis that **sectoral strength drives long-term investment opportunities**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aade14d-e172-4d38-903c-e3e4ee727ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Extract sector names directly from the strong_sectors DataFrame\n",
    "sector_list = strong_sectors.index.tolist()\n",
    "\n",
    "# Filter for only selected sectors\n",
    "df_selected = df_all[df_all['Sector'].isin(sector_list)].copy()\n",
    "\n",
    "# Ensure 'Date' column is datetime\n",
    "df_selected['Date'] = pd.to_datetime(df_selected['Date'])\n",
    "\n",
    "# Optional: filter by date range (auto from 2015 onward)\n",
    "if not df_selected.empty:\n",
    "    min_date = df_selected['Date'].min()\n",
    "    cutoff_date = pd.Timestamp(\"2015-01-01\")\n",
    "    df_selected = df_selected[df_selected['Date'] >= cutoff_date]\n",
    "\n",
    "    # Pivot the data: one column per Ticker, rows by Date\n",
    "    pivot_df = df_selected.pivot(index='Date', columns='Ticker', values='Price')\n",
    "\n",
    "    # Create interactive line plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for ticker in pivot_df.columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pivot_df.index,\n",
    "            y=pivot_df[ticker],\n",
    "            mode='lines',\n",
    "            name=ticker,\n",
    "            opacity=0.7\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Stock Price Trends (2015 to Present) – Selected Sectors',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Closing Price (INR)',\n",
    "        template='plotly_white',\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"⚠️ No data found for the selected sectors in df_all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e50447-b605-47f2-9d2d-5f982140a7c2",
   "metadata": {},
   "source": [
    "### Stock Price Trends (2015 to Present) – Selected Sectors  \n",
    "\n",
    "The above plot shows the stock price movement of selected Indian companies across sectors such as Banking, Pharma, FMCG, Auto, and Infrastructure.  \n",
    "\n",
    "\n",
    "#### Interpretation of Results:\n",
    "- **Strong Outperformers**:  \n",
    "  - **BAJFINANCE.NS** and **BAJAJFINSV.NS** show the steepest upward trajectory, reflecting strong growth in the NBFC sector.  \n",
    "  - **ADANIPORTS.NS** also demonstrates significant long-term appreciation.  \n",
    "\n",
    "- **Banking Sector**:  \n",
    "  - Large-cap banks such as **AXISBANK.NS** and **BANKBARODA.NS** have shown steady but less aggressive growth compared to NBFCs.  \n",
    "\n",
    "- **Stable Performers**:  \n",
    "  - Stocks like **ABB.NS** and **APOLLOHOSP.NS** display moderate upward movement, showing steady sectoral performance.  \n",
    "\n",
    "- **Overall Trend**:  \n",
    "  - Most companies show consistent appreciation since 2015, with sharper rallies visible post-2020, especially in finance-related stocks.  \n",
    "  - This highlights sectoral divergence — financial services have outperformed traditional industrials and pharma in the observed period.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ccbcf-0a6a-4dd4-9f07-f82893325608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Extract selected sector names\n",
    "sector_list = strong_sectors.index.tolist()\n",
    "\n",
    "# Filter dataset\n",
    "df_selected = df_all[df_all['Sector'].isin(sector_list)].copy()\n",
    "df_selected['Date'] = pd.to_datetime(df_selected['Date'])\n",
    "\n",
    "# Sector-wise plots\n",
    "for sector in sector_list:\n",
    "    df_sector = df_selected[df_selected['Sector'] == sector]\n",
    "    pivot_sector = df_sector.pivot(index='Date', columns='Ticker', values='Price')\n",
    "    \n",
    "    if pivot_sector.empty:\n",
    "        continue\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    for ticker in pivot_sector.columns:\n",
    "        fig.add_trace(go.Scatter(x=pivot_sector.index, y=pivot_sector[ticker],\n",
    "                                 mode='lines', name=ticker, opacity=0.8))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Stock Price Trends – {sector}',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Closing Price (INR)',\n",
    "        template='plotly_white',\n",
    "        height=500\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b708e4-13c7-411e-9d01-d199e951e221",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------- Performance Metrics ----------------\n",
    "# Calculate total return for each company\n",
    "performance = (\n",
    "    df_selected.groupby([\"Sector\", \"Ticker\"])[\"Price\"]\n",
    "    .agg([\"first\", \"last\"])\n",
    "    .assign(ReturnPct=lambda x: (x[\"last\"] - x[\"first\"]) / x[\"first\"] * 100)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Sort companies within each sector by return\n",
    "performance_sorted = performance.sort_values([\"Sector\", \"ReturnPct\"], ascending=[True, False])\n",
    "\n",
    "print(performance_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3976a60-7473-42d2-a428-18386794d247",
   "metadata": {},
   "source": [
    "### Performance Metrics\n",
    "\n",
    "This cell calculates and ranks the performance of companies within each sector based on their price changes.\n",
    "\n",
    "1. **Calculate Total Return for Each Company**\n",
    "   - The data is grouped by `Sector` and `Ticker`.\n",
    "   - For each company, the first and last prices are extracted.\n",
    "   - The percentage return is computed as:  \n",
    "     **ReturnPct = ((last price - first price) / first price) × 100**\n",
    "\n",
    "2. **Sort Companies by Return**\n",
    "   - Companies are sorted within each sector in descending order of their returns, so the best-performing companies appear first.\n",
    "\n",
    "3. **Output**\n",
    "   - The resulting `performance_sorted` DataFrame shows `Sector`, `Ticker`, `first price`, `last price`, and `ReturnPct` for each company in an organized manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b3141b-a85d-4479-8d08-e12a78e19957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Assuming df_all already loaded with columns: Date, Sector, Ticker, Price\n",
    "df_all['Date'] = pd.to_datetime(df_all['Date'])\n",
    "\n",
    "# ---- Step 1. Calculate daily returns ----\n",
    "df_all['Return'] = df_all.groupby('Ticker')['Price'].pct_change()\n",
    "\n",
    "# ---- Step 2. Aggregate metrics ----\n",
    "metrics = []\n",
    "for ticker, group in df_all.groupby('Ticker'):\n",
    "    sector = group['Sector'].iloc[0]\n",
    "    \n",
    "    # Drop NA returns\n",
    "    returns = group['Return'].dropna()\n",
    "    \n",
    "    if len(returns) == 0:\n",
    "        continue\n",
    "    \n",
    "    # CAGR (approx from first & last price)\n",
    "    start_price = group['Price'].iloc[0]\n",
    "    end_price = group['Price'].iloc[-1]\n",
    "    n_years = (group['Date'].iloc[-1] - group['Date'].iloc[0]).days / 365\n",
    "    if n_years <= 0:\n",
    "        continue\n",
    "    cagr = (end_price / start_price) ** (1 / n_years) - 1\n",
    "    \n",
    "    # Volatility (annualized)\n",
    "    volatility = returns.std() * np.sqrt(252)\n",
    "    \n",
    "    # Sharpe Ratio (risk-free assumed 0)\n",
    "    sharpe = (returns.mean() * 252) / (returns.std() * np.sqrt(252))\n",
    "    \n",
    "    metrics.append([ticker, sector, cagr * 100, volatility * 100, sharpe])\n",
    "\n",
    "# ---- Step 3. Build DataFrame ----\n",
    "metrics_df = pd.DataFrame(metrics, columns=['Ticker', 'Sector', 'CAGR %', 'Volatility %', 'Sharpe Ratio'])\n",
    "\n",
    "print(\"✅ metrics_df created\")\n",
    "print(metrics_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d785fc5-cb4a-4074-92f7-7afab99ecc84",
   "metadata": {},
   "source": [
    "### Stock Performance Metrics Calculation\n",
    "\n",
    "In this cell, we calculated key performance metrics for each stock in our dataset to analyze their historical performance and risk profile.  \n",
    "\n",
    "1. **Daily Returns**:  \n",
    "   - For each stock (`Ticker`), we computed the daily percentage change in price.  \n",
    "   - This gives a sense of how volatile the stock is day-to-day.\n",
    "\n",
    "2. **CAGR (Compound Annual Growth Rate)**:  \n",
    "   - Using the first and last price along with the time period, we estimated the annual growth rate.  \n",
    "   - This shows the average yearly return, smoothing out daily fluctuations.\n",
    "\n",
    "3. **Volatility**:  \n",
    "   - Calculated as the annualized standard deviation of daily returns.  \n",
    "   - Higher volatility means the stock price swings more, indicating higher risk.\n",
    "\n",
    "4. **Sharpe Ratio**:  \n",
    "   - Measures risk-adjusted return, assuming a risk-free rate of zero.  \n",
    "   - Higher Sharpe ratios indicate better return per unit of risk.\n",
    "\n",
    "5. **Result**:  \n",
    "   - All these metrics are compiled into the `metrics_df` DataFrame with columns: `Ticker`, `Sector`, `CAGR %`, `Volatility %`, `Sharpe Ratio`.  \n",
    "   - The output shows the performance and risk characteristics for each stock, helping us identify well-performing and stable investments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6057e1e-281b-48a7-8346-4aa22e6e03de",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_companies = pick_companies_from_sectors(metrics_df, strong_sectors)\n",
    "\n",
    "print(type(final_companies))  # should show <class 'pandas.core.frame.DataFrame'>\n",
    "print(final_companies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478243bd-d57f-458f-bb5a-d0eb5c182003",
   "metadata": {},
   "source": [
    "### Selecting Top Companies from Strong Sectors\n",
    "\n",
    "In this cell, I filtered and selected companies from previously identified **strong sectors** using the `pick_companies_from_sectors` function.\n",
    "\n",
    "1. **Purpose**:  \n",
    "   - Focus only on companies within sectors that are performing well overall.  \n",
    "   - Narrow down the list to high-potential stocks based on both **returns** and **risk-adjusted performance**.\n",
    "\n",
    "2. **Output**:  \n",
    "   - `final_companies` is a DataFrame containing the top companies from the strong sectors.  \n",
    "   - Columns include:\n",
    "     - `Ticker` – Stock symbol\n",
    "     - `Sector` – Sector name\n",
    "     - `CAGR %` – Annualized growth rate\n",
    "     - `Volatility %` – Annualized price fluctuation\n",
    "     - `Sharpe Ratio` – Risk-adjusted return\n",
    "\n",
    "3. **Interpretation of Results**:  \n",
    "   - Example: `IRFC.NS` in **Financial Services** has the highest CAGR of ~47.4% and a Sharpe Ratio of 1.15, indicating strong growth with reasonable risk.  \n",
    "   - `VBL.NS` and `BAJFINANCE.NS` are also top performers in **FMCG** and **Financial Services** respectively.  \n",
    "   - Companies like `TORNTPHARM.NS` and `DIVISLAB.NS` in **Healthcare** show moderate growth (~20%) but lower volatility, making them relatively stable options.  \n",
    "   - The Sharpe Ratio column helps identify companies that offer the best return for the level of risk taken—the higher, the better.  \n",
    "\n",
    "Overall, this selection highlights **high-growth and well-balanced companies** across strong sectors, useful for portfolio construction or focused analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9144bfc5-c4e3-420c-ade0-7633d46a4cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(\n",
    "    data=final_companies,\n",
    "    x=\"Ticker\",\n",
    "    y=\"Sharpe Ratio\",\n",
    "    hue=\"Sector\",\n",
    "    dodge=False,\n",
    "    palette=\"Set2\"\n",
    ")\n",
    "plt.title(\"Selected Companies by Sharpe Ratio\")\n",
    "plt.ylabel(\"Sharpe Ratio\")\n",
    "plt.xlabel(\"Company Ticker\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Sector\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2b7cca-2e41-45aa-b3c8-3d4154716daa",
   "metadata": {},
   "source": [
    "### Visualizing Selected Companies by Sharpe Ratio\n",
    "\n",
    "This cell created a bar chart to compare the **risk-adjusted performance** of the selected companies (`final_companies`) across strong sectors.\n",
    "\n",
    "- **X-axis**: `Ticker` – the stock symbol of each company.  \n",
    "- **Y-axis**: `Sharpe Ratio` – higher values indicate better return relative to risk.  \n",
    "- **Hue/Color**: `Sector` – different sectors are color-coded for easy comparison.  \n",
    "- **Purpose**:  \n",
    "  - Quickly identify which companies provide the best return per unit of risk.  \n",
    "  - Compare risk-adjusted performance across sectors.   \n",
    "\n",
    "The resulting chart helps visually spot top-performing companies and sector trends in terms of **risk-adjusted returns**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b30ed3d-8259-4d6c-b327-53611b3762e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(\n",
    "    data=final_companies,\n",
    "    x=\"Volatility %\",\n",
    "    y=\"CAGR %\",\n",
    "    size=\"Sharpe Ratio\",\n",
    "    hue=\"Sector\",\n",
    "    palette=\"Set2\",\n",
    "    sizes=(100, 500),\n",
    "    alpha=0.8\n",
    ")\n",
    "for i in range(final_companies.shape[0]):\n",
    "    plt.text(final_companies[\"Volatility %\"].iloc[i]+0.2, \n",
    "             final_companies[\"CAGR %\"].iloc[i],\n",
    "             final_companies[\"Ticker\"].iloc[i],\n",
    "             fontsize=9)\n",
    "plt.title(\"Risk vs Return of Selected Companies\")\n",
    "plt.xlabel(\"Volatility (%)\")\n",
    "plt.ylabel(\"CAGR (%)\")\n",
    "plt.legend(title=\"Sector\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfd04cb-d9ed-416b-b2c2-f3bbbaf088e8",
   "metadata": {},
   "source": [
    "### Risk vs Return of Selected Companies\n",
    "\n",
    "This cell creates a scatter plot to visualize the relationship between **risk** and **return** for the selected companies (`final_companies`).\n",
    "\n",
    "- **X-axis**: `Volatility %` – annualized risk of the stock.  \n",
    "- **Y-axis**: `CAGR %` – annualized return of the stock.  \n",
    "- **Size of Points**: `Sharpe Ratio` – larger points indicate higher risk-adjusted performance.  \n",
    "- **Color/Hue**: `Sector` – different sectors are color-coded for easy comparison.  \n",
    "\n",
    "**Additional Features**:  \n",
    "- Each point is labeled with the company `Ticker` for easy identification.  \n",
    "- `alpha=0.8` makes points slightly transparent for better visibility.  \n",
    "- Legend is positioned outside the plot for clarity.  \n",
    "\n",
    "**Purpose & Interpretation**:  \n",
    "- Helps visualize the trade-off between **risk (volatility)** and **return (CAGR)**.  \n",
    "- Companies in the **top-left** area (low volatility, high return) are attractive.  \n",
    "- Point size highlights companies with higher **risk-adjusted returns (Sharpe Ratio)**.  \n",
    "- Sector colors allow comparison of performance trends across industries.  \n",
    "\n",
    "This plot provides a quick overview of which companies combine **strong growth** with **manageable risk**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77778e-befd-4275-b195-1f92f7880504",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_companies[\"Sector\"].value_counts().plot.pie(\n",
    "    autopct='%1.1f%%',\n",
    "    figsize=(6,6),\n",
    "    colors=sns.color_palette(\"Set2\"),\n",
    "    startangle=140\n",
    ")\n",
    "plt.title(\"Sector Allocation of Selected Companies\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6565ef-235a-4411-9340-74b7f0a171e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_stats = final_companies.groupby(\"Sector\")[[\"CAGR %\", \"Volatility %\", \"Sharpe Ratio\"]].mean()\n",
    "print(sector_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8a46e0-6c96-44f6-9913-99142efa5e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_companies = len(final_companies)\n",
    "final_companies[\"Weight\"] = 1 / num_companies\n",
    "\n",
    "# Portfolio expected return (CAGR weighted)\n",
    "portfolio_cagr = (final_companies[\"CAGR %\"] * final_companies[\"Weight\"]).sum()\n",
    "\n",
    "# Portfolio expected volatility (approx, assuming uncorrelated)\n",
    "portfolio_vol = ((final_companies[\"Volatility %\"]**2 * final_companies[\"Weight\"]**2).sum())**0.5\n",
    "\n",
    "# Portfolio Sharpe Ratio (simplified)\n",
    "portfolio_sharpe = portfolio_cagr / portfolio_vol\n",
    "\n",
    "print(f\"Equal-Weighted Portfolio CAGR: {portfolio_cagr:.2f}%\")\n",
    "print(f\"Equal-Weighted Portfolio Volatility: {portfolio_vol:.2f}%\")\n",
    "print(f\"Equal-Weighted Portfolio Sharpe Ratio: {portfolio_sharpe:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6149a1d4-1f9f-412c-b05d-e8c3abe713a8",
   "metadata": {},
   "source": [
    "### Equal-Weighted Portfolio Performance\n",
    "\n",
    "I constructed an **equal-weighted portfolio** using the selected companies, assigning each company an equal weight of **1 divided by the total number of companies**.\n",
    "\n",
    "- **Portfolio Expected Return (CAGR):** 29.81%  \n",
    "  The weighted average of individual company CAGR values indicates strong expected growth for the portfolio.\n",
    "\n",
    "- **Portfolio Expected Volatility:** 10.84%  \n",
    "  Assuming uncorrelated assets, the portfolio exhibits relatively low risk, demonstrating diversification benefits.\n",
    "\n",
    "- **Portfolio Sharpe Ratio:** 2.75  \n",
    "  This simplified Sharpe Ratio highlights **excellent risk-adjusted returns**, showing that the portfolio efficiently balances return and risk.\n",
    "\n",
    "**Conclusion:** The equal-weighted portfolio of my selected companies is both high-returning and low-risk, further validating my company selection strategy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abe6f2f-67a1-46d8-957e-fe65b02e6b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sharpe = final_companies[\"Sharpe Ratio\"].sum()\n",
    "final_companies[\"Weight_Sharpe\"] = final_companies[\"Sharpe Ratio\"] / total_sharpe\n",
    "\n",
    "portfolio_cagr_sharpe = (final_companies[\"CAGR %\"] * final_companies[\"Weight_Sharpe\"]).sum()\n",
    "portfolio_vol_sharpe = ((final_companies[\"Volatility %\"]**2 * final_companies[\"Weight_Sharpe\"]**2).sum())**0.5\n",
    "portfolio_sharpe_weighted = portfolio_cagr_sharpe / portfolio_vol_sharpe\n",
    "\n",
    "print(f\"Sharpe-Weighted Portfolio CAGR: {portfolio_cagr_sharpe:.2f}%\")\n",
    "print(f\"Sharpe-Weighted Portfolio Volatility: {portfolio_vol_sharpe:.2f}%\")\n",
    "print(f\"Sharpe-Weighted Portfolio Sharpe Ratio: {portfolio_sharpe_weighted:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e72519e-9e22-49f4-bea1-baddf3ed4f20",
   "metadata": {},
   "source": [
    "### Sharpe-Weighted Portfolio Performance\n",
    "\n",
    "I constructed a **Sharpe-weighted portfolio** by assigning each company a weight proportional to its **Sharpe Ratio**. This approach gives higher allocation to companies with better risk-adjusted returns.\n",
    "\n",
    "- **Portfolio Expected Return (CAGR):** 31.05%  \n",
    "  Weighting by Sharpe Ratio increases the contribution of high-performing companies, resulting in higher expected returns than the equal-weighted portfolio.\n",
    "\n",
    "- **Portfolio Expected Volatility:** 11.17%  \n",
    "  While slightly higher than the equal-weighted portfolio, the risk remains moderate due to diversification.\n",
    "\n",
    "- **Portfolio Sharpe Ratio:** 2.78  \n",
    "  The Sharpe-weighted portfolio achieves **superior risk-adjusted performance**, confirming that emphasizing companies with higher Sharpe Ratios improves portfolio efficiency.\n",
    "\n",
    "**Conclusion:** By allocating weights based on Sharpe Ratios, my portfolio attains higher returns with marginally increased risk, effectively optimizing for risk-adjusted performance and further validating my company selection strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e222b-ebc9-4a9a-8e33-7e3bf7c55963",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon = 365\n",
    "forecast_dict = {}\n",
    "\n",
    "for ticker in pivot_df.columns:\n",
    "    series = pivot_df[ticker].dropna()\n",
    "\n",
    "    try:\n",
    "        model = ExponentialSmoothing(series, trend=\"add\", seasonal=None)\n",
    "        fit = model.fit()\n",
    "\n",
    "        forecast_index = pd.date_range(\n",
    "            start=pivot_df.index[-1] + pd.Timedelta(days=1),\n",
    "            periods=forecast_horizon,\n",
    "            freq=\"B\"\n",
    "        )\n",
    "        forecast = pd.Series(fit.forecast(forecast_horizon).values, index=forecast_index)\n",
    "\n",
    "        forecast_dict[ticker] = forecast\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Forecasting failed for {ticker}: {e}\")\n",
    "\n",
    "# --- Plot prices instead of returns ---\n",
    "fig = go.Figure()\n",
    "\n",
    "for ticker in pivot_df.columns:\n",
    "    # Actual historical prices\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pivot_df.index,\n",
    "        y=pivot_df[ticker],\n",
    "        mode='lines',\n",
    "        name=f\"{ticker} (Actual)\"\n",
    "    ))\n",
    "\n",
    "    # Forecasted prices\n",
    "    if ticker in forecast_dict:\n",
    "        forecast_series = forecast_dict[ticker]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=forecast_series.index,\n",
    "            y=forecast_series.values,\n",
    "            mode='lines',\n",
    "            name=f\"{ticker} (Forecast)\",\n",
    "            line=dict(dash='dash')\n",
    "        ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Stock Price Forecast (Next 90 Business Days)\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Price (INR)\",\n",
    "    template=\"plotly_white\",\n",
    "    height=650\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da3b521-a8de-4ece-8046-0d783fd246d6",
   "metadata": {},
   "source": [
    "### Stock Price Forecast (Next 365 Business Days)\n",
    "\n",
    "I forecasted the prices of selected companies using **Exponential Smoothing** with an additive trend for **365 business days** beyond the latest historical data.\n",
    "\n",
    "- **Actual prices:** solid lines  \n",
    "- **Forecasted prices:** dashed lines  \n",
    "\n",
    "**Interpretation:**  \n",
    "- Upward forecasts indicate potential future growth.  \n",
    "- Flat or declining forecasts suggest caution.  \n",
    "\n",
    "**Purpose:**  \n",
    "This forecast provides a **forward-looking view** to guide portfolio planning and investment decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd41aa-c35a-4f0d-839f-7fe4e5c6a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# === 1. Portfolio Data ===\n",
    "df_all['Date'] = pd.to_datetime(df_all['Date'])\n",
    "\n",
    "portfolio_tickers = final_companies['Ticker'].tolist()\n",
    "df_portfolio = df_all[df_all['Ticker'].isin(portfolio_tickers)].copy()\n",
    "\n",
    "# Filter last 5 years\n",
    "cutoff = pd.Timestamp.today() - pd.DateOffset(years=5)\n",
    "df_portfolio = df_portfolio[df_portfolio['Date'] >= cutoff]\n",
    "\n",
    "# Pivot into price table\n",
    "price_df = df_portfolio.pivot(index='Date', columns='Ticker', values='Price').dropna()\n",
    "\n",
    "# Portfolio value (₹5000 invested equally)\n",
    "initial_investment = 5000\n",
    "weights = np.repeat(1/len(price_df.columns), len(price_df.columns))\n",
    "normed = price_df / price_df.iloc[0]\n",
    "portfolio_value = (normed * weights).sum(axis=1) * initial_investment\n",
    "\n",
    "# === 2. Historical CAGR ===\n",
    "years = (portfolio_value.index[-1] - portfolio_value.index[0]).days / 365\n",
    "cagr = (portfolio_value.iloc[-1] / portfolio_value.iloc[0])**(1/years) - 1\n",
    "print(f\"Historical Portfolio CAGR: {cagr:.2%}\")\n",
    "\n",
    "# === 3. Projection Scenarios ===\n",
    "bear_cagr = max(0.06, cagr/2)   # Conservative\n",
    "base_cagr = cagr                # As-is\n",
    "bull_cagr = cagr + 0.07         # Optimistic\n",
    "\n",
    "scenarios = {\n",
    "    \"Bear Case\": bear_cagr,\n",
    "    \"Base Case\": base_cagr,\n",
    "    \"Bull Case\": bull_cagr\n",
    "}\n",
    "\n",
    "# Projection horizon\n",
    "future_years = [1, 3, 5]\n",
    "today = portfolio_value.index[-1]\n",
    "\n",
    "proj_df = pd.DataFrame(index=[today + pd.DateOffset(years=y) for y in future_years])\n",
    "\n",
    "for name, rate in scenarios.items():\n",
    "    proj_df[name] = [portfolio_value.iloc[-1] * ((1 + rate) ** y) for y in future_years]\n",
    "\n",
    "# === 4. Plot History + Projections ===\n",
    "fig = go.Figure()\n",
    "\n",
    "# Historical portfolio\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=portfolio_value.index,\n",
    "    y=portfolio_value.values,\n",
    "    mode='lines',\n",
    "    name=\"Portfolio (Historical)\",\n",
    "    line=dict(color=\"black\", width=2)\n",
    "))\n",
    "\n",
    "# Projection lines\n",
    "for case in scenarios.keys():\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[today] + proj_df.index.tolist(),\n",
    "        y=[portfolio_value.iloc[-1]] + proj_df[case].tolist(),\n",
    "        mode=\"lines+markers+text\",\n",
    "        text=[None] + [f\"₹{v:,.0f}\" for v in proj_df[case]],\n",
    "        textposition=\"top center\",\n",
    "        name=case\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Portfolio – Historical & Projected (₹5000 Invested)\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Portfolio Value (₹)\",\n",
    "    template=\"plotly_white\",\n",
    "    height=650\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77b02e-778e-4a29-9bd9-bec91832c665",
   "metadata": {},
   "source": [
    "# 📊 Portfolio Backtest & Projections (₹5000 Invested)\n",
    "\n",
    "### 1. Historical Analysis\n",
    "- Data considered: **Last 5 years (from 2021-01-29 to 2025-09-10)**  \n",
    "- Portfolio created from selected companies across strong sectors.  \n",
    "- Equal weights applied to each stock.  \n",
    "- **Historical CAGR:** 12.57%  \n",
    "\n",
    "### 2. Projection Scenarios\n",
    "Based on historical performance, we model 3 possible outcomes:\n",
    "\n",
    "- 🐻 **Bear Case:** Conservative CAGR (minimum 6% or half of historical CAGR)  \n",
    "- 📈 **Base Case:** CAGR equal to historical performance  \n",
    "- 🚀 **Bull Case:** Optimistic CAGR = historical + 7%  \n",
    "\n",
    "### 3. Results (Portfolio Value from ₹5000 invested)\n",
    "| Horizon | Bear Case | Base Case | Bull Case |\n",
    "|---------|-----------|-----------|-----------|\n",
    "| 1 Year  | ₹27,717   | ₹32,277   | ₹33,898   |\n",
    "| 3 Years | ₹39,707   | ₹62,705   | ₹72,635   |\n",
    "| 5 Years | ₹56,884   | ₹121,819  | ₹155,639  |\n",
    "\n",
    "### 4. Visualization\n",
    "- **Black Line:** Historical portfolio growth from 2020 to present.  \n",
    "- **Colored Lines:** Future projections (Bear, Base, Bull).  \n",
    "- Each projection shows the expected portfolio value at **1, 3, and 5 years**.  \n",
    "\n",
    "---\n",
    "\n",
    "💡 This approach demonstrates how ₹5000 invested in the chosen sectors could evolve under different market scenarios.  \n",
    "It provides investors with **clear downside protection (Bear)**, **realistic expectation (Base)**, and **optimistic growth potential (Bull)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2953374-74c9-44de-a87e-32352c989ec1",
   "metadata": {},
   "source": [
    "### ✅ Key Takeaways\n",
    "\n",
    "- The selected companies across strong sectors provide **high historical CAGR** with **moderate risk**, validated through portfolio backtesting.  \n",
    "- Both **equal-weighted** and **Sharpe-weighted** portfolios show strong **risk-adjusted returns**, with the Sharpe-weighted portfolio slightly optimizing returns.  \n",
    "- Scenario analysis (Bear, Base, Bull) provides **clear insight into potential portfolio outcomes**, helping in planning and risk management.  \n",
    "- The **stock price forecasts** and historical trends offer additional guidance for **future investment decisions**.  \n",
    "\n",
    "**Conclusion:**  \n",
    "This analysis demonstrates that careful **company selection and portfolio construction** can generate **consistent returns** while managing risk. Investors can use these insights to **make informed, data-driven decisions** for both short-term and long-term investment planning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18bfad85-aad4-4bb6-844a-1498261bb8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nbstripout\n",
      "  Downloading nbstripout-0.8.1-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: nbformat in c:\\users\\kanishka\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbstripout) (5.10.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\kanishka\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbformat->nbstripout) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\kanishka\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbformat->nbstripout) (4.22.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\kanishka\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbformat->nbstripout) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\kanishka\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nbformat->nbstripout) (5.14.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\kanishka\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=2.6->nbformat->nbstripout) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\kanishka\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=2.6->nbformat->nbstripout) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\kanishka\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=2.6->nbformat->nbstripout) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\kanishka\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonschema>=2.6->nbformat->nbstripout) (0.18.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\kanishka\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->nbstripout) (4.2.2)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\kanishka\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->nbstripout) (306)\n",
      "Downloading nbstripout-0.8.1-py2.py3-none-any.whl (16 kB)\n",
      "Installing collected packages: nbstripout\n",
      "Successfully installed nbstripout-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nbstripout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7dd712b-9c27-46dc-afb2-c7912e52ac7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (932712763.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    nbstripout your_notebook.ipynb\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "nbstripout .ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a02a4-c738-45d2-b077-efd4175f2ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
